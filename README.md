# LogPress - Semantic Log Compression System

[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](logpress/tests/)
[![Coverage](https://img.shields.io/badge/coverage-42%25-yellow.svg)](htmlcov/index.html)
[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

**Master's Thesis Research Project**: Automatic schema extraction from unstructured system logs using constraint-based parsing and semantic-aware compression.

## üéØ Research Goals

- **Automatic Schema Discovery**: Extract implicit log schemas without manual annotation
- **Semantic-Aware Compression**: Achieve 8-30√ó compression while maintaining queryability
- **Real-World Validation**: Tested on diverse log sources (2M+ entries)

## üöÄ Quick Start

### Installation

Preferred: Install from PyPI

```bash
# Install from PyPI (recommended)
pip install LogPress
```

Alternative: Docker (no Python setup required)

```bash
# Interactive mode
docker-compose -f deployment/docker-compose.yml run --rm logpress-interactive
```

From source (developer mode)

```bash
# Clone repository
git clone https://github.com/adam-bouafia/LogPress.git
cd LogPress

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\\Scripts\\activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
```

### Interactive Mode (Recommended)

```bash
# Beautiful terminal UI with dataset auto-discovery
python -m logpress.cli.interactive
```

**Features**:
- üîç Auto-discovers datasets in `data/datasets/`
- üìä Real-time compression progress
- üé® Rich terminal UI with tables and progress bars
- ‚ö° Query compressed logs interactively

### Command-Line Usage

```bash
# Compress logs
python -m logpress compress \
  -i data/datasets/Apache/Apache_full.log \
  -o evaluation/compressed/apache.lsc \
  --min-support 3 \
  -m

# Query compressed logs
python -m logpress query \
  -c evaluation/compressed/apache.lsc \
  --severity ERROR \
  --limit 20

# Run full evaluation
python evaluation/run_full_evaluation.py
```

### Docker Usage

```bash
# Interactive mode (Python rich UI)
docker-compose -f deployment/docker-compose.yml run --rm logpress-interactive

# Bash menu (alternative)
docker-compose -f deployment/docker-compose.yml run --rm logpress-interactive-bash

# Run specific command
docker-compose -f deployment/docker-compose.yml run --rm logpress-cli \
  compress -i /app/data/datasets/Apache/Apache_full.log -o /app/evaluation/compressed/apache.lsc -m
```

### Pre-built Docker Image

We publish pre-built Docker images to the GitHub Container Registry (GHCR). Use the following commands to pull and run the latest image:

```bash
# Pull the image from GHCR
docker pull ghcr.io/adam-bouafia/logpress:latest

# Run the CLI (example: show version)
docker run --rm ghcr.io/adam-bouafia/logpress:latest python -m logpress --version

# Run a compress command using the GHCR image
docker run --rm \
  -v "$(pwd)/data:/app/data:ro" \
  -v "$(pwd)/evaluation:/app/evaluation:rw" \
  ghcr.io/adam-bouafia/logpress:latest \
  compress -i /app/data/datasets/Apache/Apache_full.log -o /app/evaluation/compressed/apache.lsc -m
```

If you prefer Docker Hub, you or the CI workflow can mirror the image to Docker Hub with the `adambouafia/logpress:latest` tag. For example:

```bash
# (Optional) Tag and push to Docker Hub (requires Docker Hub credentials)
docker tag ghcr.io/adam-bouafia/logpress:latest adambouafia/logpress:latest
docker login --username <docker-hub-username>
docker push adambouafia/logpress:latest
```

## üìÅ Project Structure (MCP Architecture)

```
LogPress/
‚îú‚îÄ‚îÄ logpress/                  # Core Python package (Model-Context-Protocol)
‚îÇ   ‚îú‚îÄ‚îÄ models/             # Data structures (Token, LogTemplate, CompressedLog)
‚îÇ   ‚îú‚îÄ‚îÄ protocols/          # Abstract interfaces (EncoderProtocol, CompressorProtocol)
‚îÇ   ‚îú‚îÄ‚îÄ context/           # Business logpress
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tokenization/  # Smart log tokenization (FSM-based)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ extraction/    # Template generation (log alignment algorithm)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classification/# Semantic type recognition (pattern-based)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ encoding/      # Compression codecs (delta, dictionary, varint)
‚îÇ   ‚îú‚îÄ‚îÄ services/          # High-level orchestration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compressor.py  # 6-stage compression pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ query_engine.py# Queryable decompression
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ evaluator.py   # Accuracy metrics vs ground truth
‚îÇ   ‚îú‚îÄ‚îÄ cli/              # User interfaces
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interactive.py # Rich terminal UI
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands.py    # Click-based CLI
‚îÇ   ‚îî‚îÄ‚îÄ tests/            # Test suite (25 tests, 100% passing)
‚îÇ       ‚îú‚îÄ‚îÄ unit/         # Component testing
‚îÇ       ‚îú‚îÄ‚îÄ integration/  # Workflow testing
‚îÇ       ‚îú‚îÄ‚îÄ e2e/          # End-to-end testing
‚îÇ       ‚îî‚îÄ‚îÄ performance/  # Benchmarks
‚îÇ
‚îú‚îÄ‚îÄ data/                  # Input data
‚îÇ   ‚îú‚îÄ‚îÄ datasets/         # 8 real-world log sources (~1.07M entries)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Apache/       # Web server logs (52K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HealthApp/    # Android health tracking (212K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HPC/          # High-performance computing cluster logs (433K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Linux/        # Linux system logs (26K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Mac/          # macOS system logs (117K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ OpenStack/    # Cloud infrastructure logs (137K lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Proxifier/    # Network proxy logs (21K lines)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Zookeeper/    # Distributed coordination logs (74K lines)
‚îÇ   ‚îî‚îÄ‚îÄ ground_truth/     # Manual annotations for validation
‚îÇ
‚îú‚îÄ‚îÄ evaluation/           # Outputs & results
‚îÇ   ‚îú‚îÄ‚îÄ compressed/       # .lsc compressed files
‚îÇ   ‚îú‚îÄ‚îÄ results/          # Evaluation metrics (JSON/Markdown)
‚îÇ   ‚îî‚îÄ‚îÄ schema_versions/  # Schema evolution tracking
‚îÇ
‚îú‚îÄ‚îÄ deployment/          # Infrastructure
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile       # Container image
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml# Service orchestration
‚îÇ   ‚îî‚îÄ‚îÄ Makefile         # Build automation
‚îÇ
‚îú‚îÄ‚îÄ documentation/       # Project documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md        # Documentation index
‚îÇ   ‚îú‚îÄ‚îÄ TESTING.md       # Test strategy
‚îÇ   ‚îú‚îÄ‚îÄ MCP_ARCHITECTURE.md # System design
‚îÇ   ‚îî‚îÄ‚îÄ API.md           # Python API reference
‚îÇ
‚îî‚îÄ‚îÄ scripts/            # Automation scripts
    ‚îú‚îÄ‚îÄ logpress-interactive.sh  # Bash interactive menu
    ‚îú‚îÄ‚îÄ run-tests.sh           # Test suite runner
    ‚îî‚îÄ‚îÄ run-pre-production-tests.sh # Validation
```

See individual README files in each directory for detailed information.

## üî¨ Research Methodology

### 1. Schema Extraction Pipeline

**6-Stage Process**:
1. **Tokenization**: FSM-based parser handles diverse log formats
2. **Semantic Classification**: Pattern-based field type detection (timestamp, IP, severity, etc.)
3. **Field Grouping**: Identify related fields (ip+port, user+action)
4. **Template Generation**: Log alignment algorithm extracts schemas
5. **Schema Versioning**: Track format evolution over time
6. **Validation**: Compare against manual ground truth (precision/recall)

**Example**:
```
Raw Logs:
  [Thu Jun 09 06:07:04 2005] [notice] LDAP: Built with OpenLDAP
  [Thu Jun 09 06:07:05 2005] [notice] LDAP: SSL support unavailable
  
Extracted Template:
  [TIMESTAMP] [SEVERITY] LDAP: [MESSAGE]
```

### 2. Semantic-Aware Compression

**Category-Specific Codecs**:
- **Timestamps**: Delta encoding (8-10√ó compression)
- **Severity/Status**: Dictionary encoding (5-7√ó compression)
- **Metrics**: Gorilla time-series compression (3-5√ó compression)
- **Messages**: Token pool with references (variable)
- **Stack traces**: Reference tracking (store once, reuse pointer)

**Queryable Index**: Columnar storage enables filtering without full decompression.

### 3. Evaluation Metrics

**Accuracy** (vs manual annotations):
- Precision: % of extracted fields that are correct
- Recall: % of actual fields that were found
- F1-Score: Harmonic mean
- **Target**: >90% accuracy

**Compression Performance**:
- Compression ratio vs gzip baseline
- Query latency overhead
- **Target**: >10√ó compression, <2√ó query slowdown

## üß™ Testing

### Run Complete Test Suite

```bash
# All tests with coverage
bash scripts/run-tests.sh

# View coverage report
firefox htmlcov/index.html
```

### Pre-Production Validation

```bash
# Validate before deployment
bash scripts/run-pre-production-tests.sh
```

**Test Status**: ‚úÖ 25/25 tests passing (100%)
- Unit tests: 9 tests
- Integration tests: 8 tests
- E2E tests: 3 tests
- Performance benchmarks: 5 tests

### Performance Benchmarks

```bash
# Run benchmarks
python -m pytest logpress/tests/performance/ --benchmark-only

# Expected results:
# - Compression: >500 ops/sec
# - Template extraction: >900 ops/sec
# - Linear scalability: 100 ‚Üí 10,000 logs
```

## üìö Documentation

- [Documentation Index](documentation/README.md) - Complete documentation overview
- [Testing Guide](documentation/TESTING.md) - Test strategy and commands
- [MCP Architecture](documentation/MCP_ARCHITECTURE.md) - System design details
- [API Reference](documentation/API.md) - Python API usage
- [Docker Guide](deployment/README.md) - Container deployment

## üéì Research Context

**Master's Thesis**: Automatic Schema Extraction from Unstructured System Logs  
**Duration**: 26 weeks (4 phases)  
**Target Venues**: VLDB, SIGMOD, IEEE BigData  
**Novel Contribution**: Semantic-aware compression adapting to log content types

### Related Work
- **Log Parsing**: Drain, Spell, LogPai
- **Schema Inference**: Lakehouse formats (Parquet, ORC)
- **Compression**: Generic (gzip, zstd) vs specialized (LogShrink)

### Key Differentiators
- ‚úÖ No ML models (constraint-based approach)
- ‚úÖ Semantic awareness (field-type-specific compression)
- ‚úÖ Query preservation (columnar indexes)
- ‚úÖ Schema evolution tracking
- ‚úÖ Lossless compression (exact reconstruction)

## üõ†Ô∏è Development

### Setup Development Environment

```bash
# Install test dependencies
pip install pytest pytest-cov pytest-benchmark pytest-mock

# Run tests on file changes (watch mode)
pip install pytest-watch
ptw logpress/tests/ -- -v
```

### Contribution Workflow

1. Create feature branch: `git checkout -b feature/new-encoder`
2. Make changes and add tests
3. Run validation: `bash scripts/run-pre-production-tests.sh`
4. Submit PR (GitHub Actions runs full test suite)

### Adding New Semantic Type Patterns

```python
# logpress/context/classification/semantic_types.py

def recognize_custom_field(token: str) -> Tuple[str, float]:
    """
    Add pattern for new field type.
    
    Returns:
        (field_type, confidence_score)
    """
    if re.match(r'^[A-Z]{3}-\d{4}$', token):
        return ('ERROR_CODE', 0.95)  # High confidence
    return ('UNKNOWN', 0.0)
```

### Adding New Compression Codecs

```python
# logpress/context/encoding/custom_encoder.py

from logpress.protocols import EncoderProtocol

class CustomEncoder(EncoderProtocol):
    def encode(self, values: List[Any]) -> bytes:
        # Your encoding logpress
        pass
    
    def decode(self, data: bytes) -> List[Any]:
        # Your decoding logpress
        pass
```

## üì¶ Dependencies

### Core Libraries
```
msgpack>=1.0.0          # Serialization
zstandard>=0.21.0       # Compression baseline
python-dateutil>=2.8.0  # Timestamp parsing
regex>=2023.0.0         # Advanced pattern matching
rich>=13.0.0            # Terminal UI
click>=8.1.0            # CLI framework
```

### Testing
```
pytest>=7.4.0           # Test framework
pytest-cov>=4.1.0       # Coverage reporting
pytest-benchmark>=4.0.0 # Performance testing
pytest-mock>=3.12.0     # Mocking utilities
```

### Optional Tools
```bash
# Baseline comparison
gzip --version

# Command-line benchmarking
cargo install hyperfine

# Memory profiling
pip install memory-profiler
```

## üê≥ Docker Deployment

### Build & Run

```bash
# Build all services
docker-compose -f deployment/docker-compose.yml build

# Run interactive CLI
docker-compose -f deployment/docker-compose.yml run --rm logpress-interactive

# Run compression
docker-compose -f deployment/docker-compose.yml run --rm logpress-cli \
  compress -i /app/data/datasets/Apache/Apache_full.log -o /app/evaluation/compressed/apache.lsc
```

### Environment Variables

```bash
# Set in docker-compose.yml
PYTHONUNBUFFERED=1      # Real-time output
TERM=xterm-256color     # Colored terminal
MIN_SUPPORT=3           # Template extraction threshold
ZSTD_LEVEL=15           # Compression level (1-22)
```

## ü§ù Contributing

We welcome contributions! Please see CONTRIBUTING.md for guidelines.

### Areas for Contribution
- [ ] Additional semantic type patterns
- [ ] New compression codecs
- [ ] Query optimization
- [ ] Schema visualization
- [ ] Performance improvements

## üìÑ License

MIT License - see [LICENSE](LICENSE) file for details.


## üîó Links

- [Project Documentation](documentation/README.md)
- [Test Results](evaluation/results/)
- [Research Roadmap](PROJECT.md)
- [GitHub Repository](https://github.com/adam-bouafia/logpress)

## üìû Contact

- **Author**: Adam Bouafia
- **Repository**: https://github.com/adam-bouafia/logpress
- **Linkedin**: https://www.linkedin.com/in/adam-bouafia 

---

**Status**: ‚úÖ Production Ready | üß™ All Tests Passing (25/25) | üìä Coverage: 42%

Built with ‚ù§Ô∏è for research in log analysis and semantic compression.
